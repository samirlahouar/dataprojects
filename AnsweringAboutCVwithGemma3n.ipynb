{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I2_8pTj1BVq"
      },
      "outputs": [],
      "source": [
        "# Function to safely install packages\n",
        "def install_package(package, version=None):\n",
        "    \"\"\"Install a package safely, checking if it's already installed.\"\"\"\n",
        "    try:\n",
        "        print(f\"Installing {package}\")\n",
        "        if version:\n",
        "            # Use >= for minimum version specification\n",
        "            !pip install -q {package}{version}\n",
        "        else:\n",
        "            !pip install -q {package}\n",
        "        print(f\"Successfully installed {package}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to install {package}: {e}\")\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# List of required packages with versions\n",
        "required_packages = {\n",
        "    \"gradio\": \"latest\",\n",
        "    \"ollama\": \"latest\",\n",
        "    \"markdown2\": \"latest\",\n",
        "    # \"torch\": \">=2.4.0\",\n",
        "    # \"transformers\": \">=4.53.0\"\n",
        "    }\n",
        "\n",
        "# Install packages\n",
        "for package, version in required_packages.items():\n",
        "    if version == \"latest\":\n",
        "        install_package(package)\n",
        "    else:\n",
        "        install_package(package, version)\n",
        "\n",
        "print(\"!!! Installation concluded !!!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lev1FqzY1Pd0",
        "outputId": "380c1c5e-1718-4d04-b85a-643b618b9d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing gradio\n",
            "Successfully installed gradio\n",
            "Installing ollama\n",
            "Successfully installed ollama\n",
            "Installing markdown2\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSuccessfully installed markdown2\n",
            "!!! Installation concluded !!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import os\n",
        "import sys\n",
        "import psutil\n",
        "import subprocess\n",
        "import logging\n",
        "import warnings\n",
        "import gradio as gr\n",
        "import ollama\n",
        "from ollama import chat\n",
        "from PIL import Image\n",
        "import io\n",
        "import base64\n",
        "import markdown2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "536ky14L1YFI",
        "outputId": "b9af49d0-9646-4295-ae35-b52d0bcddcc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6.21 s, sys: 351 ms, total: 6.56 s\n",
            "Wall time: 10.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Cell: Logger Configuration\n",
        "# Create a logger\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create file handler and set level to debug\n",
        "file_handler = logging.FileHandler('app.log')\n",
        "file_handler.setLevel(logging.INFO)\n",
        "\n",
        "# Create console handler and set level to error\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setLevel(logging.ERROR)\n",
        "\n",
        "# Create formatters\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Add formatters to handlers\n",
        "file_handler.setFormatter(formatter)\n",
        "console_handler.setFormatter(formatter)\n",
        "\n",
        "# Add handlers to logger\n",
        "logger.addHandler(file_handler)\n",
        "logger.addHandler(console_handler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_jtKg461fU7",
        "outputId": "f5e56f79-d653-4e11-866d-d37e97f32ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 215 ¬µs, sys: 948 ¬µs, total: 1.16 ms\n",
            "Wall time: 4.61 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# _/$\\_\\%/_/&\\_\\@/_/$\\_\\%/_/&\\_\\@/_/$\\_\\%/_/&\\_\\@/_\n",
        "# ==================================================\n",
        "#  **********    System Information   *************\n",
        "# ==================================================\n",
        "# _/$\\_\\%/_/&\\_\\@/_/$\\_\\%/_/&\\_\\@/_/$\\_\\%/_/&\\_\\@/_\n",
        "# ==================================================\n",
        "def print_system_info(use_torch=False):\n",
        "    print(\"System Information:\")\n",
        "    print(f\"‚Ä¢ Python version: {sys.version}\")\n",
        "    print(f\"‚Ä¢ Current working directory: {os.getcwd()}\")\n",
        "\n",
        "    if use_torch:\n",
        "        # No need to import torch. Just use it.\n",
        "        print(f\"‚Ä¢ PyTorch version: {torch.__version__}\")\n",
        "        # Check GPU availability and details\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_info = {\n",
        "                \"CUDA Available\": torch.cuda.is_available(),\n",
        "                \"CUDA Device Count\": torch.cuda.device_count(),\n",
        "                \"Current CUDA Device\": torch.cuda.current_device(),\n",
        "                \"Device Name\": torch.cuda.get_device_name(torch.cuda.current_device()),\n",
        "                \"Memory Allocated (MB)\": round(torch.cuda.memory_allocated(0) / 1024**2, 2),\n",
        "                \"Memory Reserved (MB)\": round(torch.cuda.memory_reserved(0) / 1024**2, 2),\n",
        "            }\n",
        "\n",
        "            print(\"\\n‚ö° GPU Detected:\")\n",
        "            for key, value in gpu_info.items():\n",
        "                print(f\"  ‚Ä¢ {key}: {value}\")\n",
        "        else:\n",
        "            print(\"\\nüò≠ No GPU detected. Running on CPU only.\")\n",
        "\n",
        "    # Memory information\n",
        "    ram = psutil.virtual_memory()\n",
        "    print(\"\\nüêò System Memory:\")\n",
        "    print(f\"  ‚Ä¢ Total RAM: {round(ram.total / 1024**2, 2)} MB\")\n",
        "    print(f\"  ‚Ä¢ Available RAM: {round(ram.available / 1024**2, 2)} MB\")\n",
        "    print(f\"  ‚Ä¢ Used RAM: {round(ram.used / 1024**2, 2)} MB\")\n",
        "    print(f\"  ‚Ä¢ RAM Percentage: {ram.percent}% used\")\n",
        "\n",
        "# Check if torch is imported\n",
        "try:\n",
        "    import torch\n",
        "    torch_imported = True  # Indicate that torch is available\n",
        "except ImportError:\n",
        "    torch_imported = False  # Indicate that torch is not available\n",
        "\n",
        "# Call the function based on whether torch is imported\n",
        "if torch_imported:\n",
        "    print_system_info(use_torch=True)  # Call the function with use_torch=True\n",
        "else:\n",
        "    print_system_info(use_torch=False)  # Call the function with use_torch=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLWmAy2S1lSY",
        "outputId": "752840b4-1ad0-42dc-ac32-0e8a79cc223b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System Information:\n",
            "‚Ä¢ Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "‚Ä¢ Current working directory: /content\n",
            "‚Ä¢ PyTorch version: 2.6.0+cu124\n",
            "\n",
            "üò≠ No GPU detected. Running on CPU only.\n",
            "\n",
            "üêò System Memory:\n",
            "  ‚Ä¢ Total RAM: 12977.95 MB\n",
            "  ‚Ä¢ Available RAM: 11611.74 MB\n",
            "  ‚Ä¢ Used RAM: 1046.35 MB\n",
            "  ‚Ä¢ RAM Percentage: 10.5% used\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "#process = subprocess.Popen(\"ollama serve\", shell=True)\n",
        "\n",
        "# !ollama pull gemma3n:e4b\n",
        "# !ollama pull gemma3n:e2b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwHxiqD11vFo",
        "outputId": "9bcff0b1-b229-468d-b9a6-f2e2d2f8cf07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process = subprocess.Popen(\"ollama serve\", shell=True)\n"
      ],
      "metadata": {
        "id": "IgNtCfor2EsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull gemma3n:e2b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMNwaHRp2NLx",
        "outputId": "f111533e-e414-4e23-e3c3-3a3b2d63ef86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_share=\"\"\"I am a potential customer or a hiring manager. You are Samir LAHOUAR's assistant designed to answer on his behalf **only**  on relevant questions about me and my cv below.\n",
        "            If a question is not related to his cv or to work jobs, reply politely that you are not allowed to answer. --CV START-- I am Samir LAHOUAR : Solutions Architect | Data Engineer | SAP Specialist | Consultant CDI mon numero de tel : +337 66 41 35 87 , mon email: samirlahouar@gmail.com mon compte upwork : https://www.upwork.com/freelancers/samirlahouar je suis install√© √† Paris et je suis mobile en france et a l'√©tranger. je suis libre a partir d'aout 2025 pour travailler. J'ai fait plus de 30 projets sur upwork les plus importants : Data Engineer ‚Äì Epum\n",
        "2024 ‚Äì Maintenant\n",
        "‚Ä¢\tD√©veloppement et d√©ploiement de plus de 800 crawlers Python pour extraire des donn√©es depuis des sites web complexes, en utilisant des outils avanc√©s comme Playwright, HTTPX et BeautifulSoup.\n",
        "‚Ä¢\tOptimisation des syst√®mes internes, am√©lioration des performances et contournement des protections anti-bots, avec des contributions au framework interne pour g√©rer des structures web sophistiqu√©es.\n",
        "Solutions Architect ‚Äì Mediareps LLC\n",
        "2020 ‚Äì 2024\n",
        "‚Ä¢\tMise en place d'un syst√®me de protection des droits d'auteur (DMCAForce) : d√©tection des infractions, g√©n√©ration automatique des avis de retrait DMCA, et communication avec les FAI pour garantir la conformit√©.\n",
        "‚Ä¢\tConception d'un syst√®me RTB (Real-Time Bidding) pour la publicit√© : gestion des requ√™tes en temps r√©el, s√©lection de la meilleure offre et finalisation des transactions.\n",
        "Chef de Projet ‚Äì Auxiliary Teams\n",
        "2017 ‚Äì 2020\n",
        "‚Ä¢\tSupervision de projets pour des clients Fortune 500 et startups : d√©ploiement de solutions SaaS int√©grant API et interfaces utilisateur.\n",
        "‚Ä¢\tGestion d'√©quipes sp√©cialis√©es en IT, d√©veloppement web et mobile, avec expertise sur les plateformes cloud Linux et Windows.\n",
        "\n",
        "Enseignant Chercheur‚Äì ENIM\n",
        "2008 ‚Äì 2017\n",
        "‚Ä¢\tEnseignement en robotique, apprentissage automatique, vision par ordinateur et impression 3D.\n",
        "‚Ä¢\tRecherche en robotique, planification de trajectoires, apprentissage automatique et vision par ordinateur.\n",
        " FORMATIONS\n",
        "Mast√®re en Intelligence Artificielle et Digital Management\n",
        "IA School Paris, 2022 ‚Äì 2024\n",
        "‚Ä¢\tGestion de projets IA : m√©thodologies agiles, optimisation de la prise de d√©cision, conception de solutions data-driven.\n",
        "‚Ä¢\tAnalyse de donn√©es et interpr√©tation des tendances de march√© pour des strat√©gies marketing efficaces.\n",
        "Doctorat de robotique\n",
        "Universit√© de Poitiers, 2004 ‚Äì 2008\n",
        "‚Ä¢\tRecherche sur la robotique : planification de trajectoires et apprentissage automatique.\n",
        "‚Ä¢\tMaitrise de C/C++, Linux et Temps R√©el (RT). OpenCV, ROS, Matlab/Simulink avec utilisation des protocoles de communication CAN, Modbus, RS232/485\n",
        "\n",
        "\n",
        "\n",
        "COMP√âTENCES TECHNIQUES\n",
        "Langages de programmation : Python, R, Java, JavaScript, PHP, Bash, Perl\n",
        "Bases de donn√©es : SQL, MySQL, MongoDB, DynamoDB, Elasticsearch\n",
        "Outils de visualisation : Power BI, Kibana, Grafana, Tableau, Matlab\n",
        "Cloud Platforms : AWS (EC2, S3, Lambda, CloudFormation, API Gateway), Azure, GCP\n",
        "DevOps & CI/CD : GitLab CI/CD, Docker Compose, Jenkins, Github Actions\n",
        "Machine Learning : Scikit-learn, TensorFlow, PyTorch, Keras, Pandas, NumPy, Matplotlib\n",
        "\n",
        "CERTIFICATIONS\n",
        "‚Ä¢\tDatacamp Data Engineer Associate Certificate\n",
        "‚Ä¢\tMongoDB Data Modeling (M320)\n",
        "‚Ä¢\tMongoDB for Python Developers (M220P)\n",
        "\n",
        "\n",
        "PROJETS\n",
        "Syst√®me de r√©ponse automatique\n",
        "‚Ä¢\tD√©veloppement d‚Äôun syst√®me bas√© sur FreeSWITCH et l‚ÄôAPI ChatGPT pour r√©pondre aux requ√™tes clients, augmentant la satisfaction client gr√¢ce √† des r√©ponses contextuelles et rapides.\n",
        "Mod√®le de segmentation d‚ÄôIRM prostatique\n",
        "‚Ä¢\tCr√©ation d‚Äôun mod√®le UNet ajust√© pour la segmentation de la prostate, valid√© sur diff√©rentes m√©triques et r√©solutions pr√©cises.\n",
        "\n",
        "INFORMATIONS SUPPL√âMENTAIRES\n",
        "‚Ä¢\tLangues : Fran√ßais (courant), Anglais (professionnel) --CV END--\"\"\""
      ],
      "metadata": {
        "id": "5XRVN2J7V5Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_share=\"\"\n",
        "# Unified function for chat with Gemma\n",
        "def gemma_chat(history, url_input, question):\n",
        "    try:\n",
        "        # Validate the question input\n",
        "        question = question.strip()\n",
        "        if not question:\n",
        "            return history, \"Please enter a valid question.\"\n",
        "        if url_input:  # If a URL has been provided\n",
        "            message_content = f\"You have received a URL for a file: {url_input}. Question: {question}\"\n",
        "        else:  # No URL provided, just process the question\n",
        "            message_content = f\"{data_to_share} Question: {question}\"\n",
        "        # Send the prompt to the Ollama model\n",
        "        response = chat(model='gemma3n:e2b', messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": message_content\n",
        "            },\n",
        "        ])\n",
        "        answer = response['message']['content']\n",
        "        # Convert the Markdown answer to HTML\n",
        "        answer_html = markdown2.markdown(answer)\n",
        "        # Update history with the new interaction\n",
        "        history.append(f\"<div style='color: blue;'>You: {question}</div>\")\n",
        "        history.append(f\"<div style='color: green;'>Gemma 3n: {answer_html}</div>\")\n",
        "        history_text = \"<br>\".join(history)\n",
        "\n",
        "        return history_text, answer\n",
        "    except Exception as e:\n",
        "        return history, f\"Error occurred: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "GbbBvHN73O08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    history = gr.State([])\n",
        "    with gr.Column():\n",
        "        gr.Markdown(\"# Welcome to Samir LAHOUAR Portal\")\n",
        "        chat_output = gr.HTML(label=\"Chat History\")\n",
        "        response_output = gr.Textbox(label=\"Response\", placeholder=\"Model response will appear here...\", interactive=False)\n",
        "        url_input = gr.Textbox(lines=1, label=\"Enter File URL (audio/image) or leave it blank\")\n",
        "        question_input = gr.Textbox(lines=2, label=\"Ask Samir's Assistant\")\n",
        "\n",
        "        submit_button = gr.Button(\"Submit\")\n",
        "\n",
        "    # Connect inputs and outputs\n",
        "    submit_button.click(\n",
        "        gemma_chat,\n",
        "        inputs=[history, url_input, question_input],\n",
        "        outputs=[chat_output, response_output]\n",
        "    )"
      ],
      "metadata": {
        "id": "04VNst8x3bcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Launch the Gradio interface\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "yrYx95sS3jNP",
        "outputId": "d28e2dfc-23c6-4dd0-8cfa-ba95d3588042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://4845863e6d8759503c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4845863e6d8759503c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}