{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I2_8pTj1BVq"
      },
      "outputs": [],
      "source": [
        "# Function to safely install packages\n",
        "def install_package(package, version=None):\n",
        "    \"\"\"Install a package safely, checking if it's already installed.\"\"\"\n",
        "    try:\n",
        "        print(f\"Installing {package}\")\n",
        "        if version:\n",
        "            # Use >= for minimum version specification\n",
        "            !pip install -q {package}{version}\n",
        "        else:\n",
        "            !pip install -q {package}\n",
        "        print(f\"Successfully installed {package}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to install {package}: {e}\")\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# List of required packages with versions\n",
        "required_packages = {\n",
        "    \"gradio\": \"latest\",\n",
        "    \"ollama\": \"latest\",\n",
        "    \"markdown2\": \"latest\",\n",
        "    # \"torch\": \">=2.4.0\",\n",
        "    # \"transformers\": \">=4.53.0\"\n",
        "    }\n",
        "\n",
        "# Install packages\n",
        "for package, version in required_packages.items():\n",
        "    if version == \"latest\":\n",
        "        install_package(package)\n",
        "    else:\n",
        "        install_package(package, version)\n",
        "\n",
        "print(\"!!! Installation concluded !!!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lev1FqzY1Pd0",
        "outputId": "380c1c5e-1718-4d04-b85a-643b618b9d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing gradio\n",
            "Successfully installed gradio\n",
            "Installing ollama\n",
            "Successfully installed ollama\n",
            "Installing markdown2\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSuccessfully installed markdown2\n",
            "!!! Installation concluded !!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import os\n",
        "import sys\n",
        "import psutil\n",
        "import subprocess\n",
        "import logging\n",
        "import warnings\n",
        "import gradio as gr\n",
        "import ollama\n",
        "from ollama import chat\n",
        "from PIL import Image\n",
        "import io\n",
        "import base64\n",
        "import markdown2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "536ky14L1YFI",
        "outputId": "b9af49d0-9646-4295-ae35-b52d0bcddcc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6.21 s, sys: 351 ms, total: 6.56 s\n",
            "Wall time: 10.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Cell: Logger Configuration\n",
        "# Create a logger\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create file handler and set level to debug\n",
        "file_handler = logging.FileHandler('app.log')\n",
        "file_handler.setLevel(logging.INFO)\n",
        "\n",
        "# Create console handler and set level to error\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setLevel(logging.ERROR)\n",
        "\n",
        "# Create formatters\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Add formatters to handlers\n",
        "file_handler.setFormatter(formatter)\n",
        "console_handler.setFormatter(formatter)\n",
        "\n",
        "# Add handlers to logger\n",
        "logger.addHandler(file_handler)\n",
        "logger.addHandler(console_handler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_jtKg461fU7",
        "outputId": "f5e56f79-d653-4e11-866d-d37e97f32ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 215 µs, sys: 948 µs, total: 1.16 ms\n",
            "Wall time: 4.61 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# _/$\\_\\%/_/&\\_\\@/_/$\\_\\%/_/&\\_\\@/_/$\\_\\%/_/&\\_\\@/_\n",
        "# ==================================================\n",
        "#  **********    System Information   *************\n",
        "# ==================================================\n",
        "# _/$\\_\\%/_/&\\_\\@/_/$\\_\\%/_/&\\_\\@/_/$\\_\\%/_/&\\_\\@/_\n",
        "# ==================================================\n",
        "def print_system_info(use_torch=False):\n",
        "    print(\"System Information:\")\n",
        "    print(f\"• Python version: {sys.version}\")\n",
        "    print(f\"• Current working directory: {os.getcwd()}\")\n",
        "\n",
        "    if use_torch:\n",
        "        # No need to import torch. Just use it.\n",
        "        print(f\"• PyTorch version: {torch.__version__}\")\n",
        "        # Check GPU availability and details\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_info = {\n",
        "                \"CUDA Available\": torch.cuda.is_available(),\n",
        "                \"CUDA Device Count\": torch.cuda.device_count(),\n",
        "                \"Current CUDA Device\": torch.cuda.current_device(),\n",
        "                \"Device Name\": torch.cuda.get_device_name(torch.cuda.current_device()),\n",
        "                \"Memory Allocated (MB)\": round(torch.cuda.memory_allocated(0) / 1024**2, 2),\n",
        "                \"Memory Reserved (MB)\": round(torch.cuda.memory_reserved(0) / 1024**2, 2),\n",
        "            }\n",
        "\n",
        "            print(\"\\n⚡ GPU Detected:\")\n",
        "            for key, value in gpu_info.items():\n",
        "                print(f\"  • {key}: {value}\")\n",
        "        else:\n",
        "            print(\"\\n😭 No GPU detected. Running on CPU only.\")\n",
        "\n",
        "    # Memory information\n",
        "    ram = psutil.virtual_memory()\n",
        "    print(\"\\n🐘 System Memory:\")\n",
        "    print(f\"  • Total RAM: {round(ram.total / 1024**2, 2)} MB\")\n",
        "    print(f\"  • Available RAM: {round(ram.available / 1024**2, 2)} MB\")\n",
        "    print(f\"  • Used RAM: {round(ram.used / 1024**2, 2)} MB\")\n",
        "    print(f\"  • RAM Percentage: {ram.percent}% used\")\n",
        "\n",
        "# Check if torch is imported\n",
        "try:\n",
        "    import torch\n",
        "    torch_imported = True  # Indicate that torch is available\n",
        "except ImportError:\n",
        "    torch_imported = False  # Indicate that torch is not available\n",
        "\n",
        "# Call the function based on whether torch is imported\n",
        "if torch_imported:\n",
        "    print_system_info(use_torch=True)  # Call the function with use_torch=True\n",
        "else:\n",
        "    print_system_info(use_torch=False)  # Call the function with use_torch=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLWmAy2S1lSY",
        "outputId": "752840b4-1ad0-42dc-ac32-0e8a79cc223b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System Information:\n",
            "• Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "• Current working directory: /content\n",
            "• PyTorch version: 2.6.0+cu124\n",
            "\n",
            "😭 No GPU detected. Running on CPU only.\n",
            "\n",
            "🐘 System Memory:\n",
            "  • Total RAM: 12977.95 MB\n",
            "  • Available RAM: 11611.74 MB\n",
            "  • Used RAM: 1046.35 MB\n",
            "  • RAM Percentage: 10.5% used\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "#process = subprocess.Popen(\"ollama serve\", shell=True)\n",
        "\n",
        "# !ollama pull gemma3n:e4b\n",
        "# !ollama pull gemma3n:e2b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwHxiqD11vFo",
        "outputId": "9bcff0b1-b229-468d-b9a6-f2e2d2f8cf07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process = subprocess.Popen(\"ollama serve\", shell=True)\n"
      ],
      "metadata": {
        "id": "IgNtCfor2EsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull gemma3n:e2b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMNwaHRp2NLx",
        "outputId": "f111533e-e414-4e23-e3c3-3a3b2d63ef86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_share=\"\"\"I am a potential customer or a hiring manager. You are Samir LAHOUAR's assistant designed to answer on his behalf **only**  on relevant questions about me and my cv below.\n",
        "            If a question is not related to his cv or to work jobs, reply politely that you are not allowed to answer. --CV START-- I am Samir LAHOUAR : Solutions Architect | Data Engineer | SAP Specialist | Consultant CDI mon numero de tel : +337 66 41 35 87 , mon email: samirlahouar@gmail.com mon compte upwork : https://www.upwork.com/freelancers/samirlahouar je suis installé à Paris et je suis mobile en france et a l'étranger. je suis libre a partir d'aout 2025 pour travailler. J'ai fait plus de 30 projets sur upwork les plus importants : Data Engineer – Epum\n",
        "2024 – Maintenant\n",
        "•\tDéveloppement et déploiement de plus de 800 crawlers Python pour extraire des données depuis des sites web complexes, en utilisant des outils avancés comme Playwright, HTTPX et BeautifulSoup.\n",
        "•\tOptimisation des systèmes internes, amélioration des performances et contournement des protections anti-bots, avec des contributions au framework interne pour gérer des structures web sophistiquées.\n",
        "Solutions Architect – Mediareps LLC\n",
        "2020 – 2024\n",
        "•\tMise en place d'un système de protection des droits d'auteur (DMCAForce) : détection des infractions, génération automatique des avis de retrait DMCA, et communication avec les FAI pour garantir la conformité.\n",
        "•\tConception d'un système RTB (Real-Time Bidding) pour la publicité : gestion des requêtes en temps réel, sélection de la meilleure offre et finalisation des transactions.\n",
        "Chef de Projet – Auxiliary Teams\n",
        "2017 – 2020\n",
        "•\tSupervision de projets pour des clients Fortune 500 et startups : déploiement de solutions SaaS intégrant API et interfaces utilisateur.\n",
        "•\tGestion d'équipes spécialisées en IT, développement web et mobile, avec expertise sur les plateformes cloud Linux et Windows.\n",
        "\n",
        "Enseignant Chercheur– ENIM\n",
        "2008 – 2017\n",
        "•\tEnseignement en robotique, apprentissage automatique, vision par ordinateur et impression 3D.\n",
        "•\tRecherche en robotique, planification de trajectoires, apprentissage automatique et vision par ordinateur.\n",
        " FORMATIONS\n",
        "Mastère en Intelligence Artificielle et Digital Management\n",
        "IA School Paris, 2022 – 2024\n",
        "•\tGestion de projets IA : méthodologies agiles, optimisation de la prise de décision, conception de solutions data-driven.\n",
        "•\tAnalyse de données et interprétation des tendances de marché pour des stratégies marketing efficaces.\n",
        "Doctorat de robotique\n",
        "Université de Poitiers, 2004 – 2008\n",
        "•\tRecherche sur la robotique : planification de trajectoires et apprentissage automatique.\n",
        "•\tMaitrise de C/C++, Linux et Temps Réel (RT). OpenCV, ROS, Matlab/Simulink avec utilisation des protocoles de communication CAN, Modbus, RS232/485\n",
        "\n",
        "\n",
        "\n",
        "COMPÉTENCES TECHNIQUES\n",
        "Langages de programmation : Python, R, Java, JavaScript, PHP, Bash, Perl\n",
        "Bases de données : SQL, MySQL, MongoDB, DynamoDB, Elasticsearch\n",
        "Outils de visualisation : Power BI, Kibana, Grafana, Tableau, Matlab\n",
        "Cloud Platforms : AWS (EC2, S3, Lambda, CloudFormation, API Gateway), Azure, GCP\n",
        "DevOps & CI/CD : GitLab CI/CD, Docker Compose, Jenkins, Github Actions\n",
        "Machine Learning : Scikit-learn, TensorFlow, PyTorch, Keras, Pandas, NumPy, Matplotlib\n",
        "\n",
        "CERTIFICATIONS\n",
        "•\tDatacamp Data Engineer Associate Certificate\n",
        "•\tMongoDB Data Modeling (M320)\n",
        "•\tMongoDB for Python Developers (M220P)\n",
        "\n",
        "\n",
        "PROJETS\n",
        "Système de réponse automatique\n",
        "•\tDéveloppement d’un système basé sur FreeSWITCH et l’API ChatGPT pour répondre aux requêtes clients, augmentant la satisfaction client grâce à des réponses contextuelles et rapides.\n",
        "Modèle de segmentation d’IRM prostatique\n",
        "•\tCréation d’un modèle UNet ajusté pour la segmentation de la prostate, validé sur différentes métriques et résolutions précises.\n",
        "\n",
        "INFORMATIONS SUPPLÉMENTAIRES\n",
        "•\tLangues : Français (courant), Anglais (professionnel) --CV END--\"\"\""
      ],
      "metadata": {
        "id": "5XRVN2J7V5Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_share=\"\"\n",
        "# Unified function for chat with Gemma\n",
        "def gemma_chat(history, url_input, question):\n",
        "    try:\n",
        "        # Validate the question input\n",
        "        question = question.strip()\n",
        "        if not question:\n",
        "            return history, \"Please enter a valid question.\"\n",
        "        if url_input:  # If a URL has been provided\n",
        "            message_content = f\"You have received a URL for a file: {url_input}. Question: {question}\"\n",
        "        else:  # No URL provided, just process the question\n",
        "            message_content = f\"{data_to_share} Question: {question}\"\n",
        "        # Send the prompt to the Ollama model\n",
        "        response = chat(model='gemma3n:e2b', messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": message_content\n",
        "            },\n",
        "        ])\n",
        "        answer = response['message']['content']\n",
        "        # Convert the Markdown answer to HTML\n",
        "        answer_html = markdown2.markdown(answer)\n",
        "        # Update history with the new interaction\n",
        "        history.append(f\"<div style='color: blue;'>You: {question}</div>\")\n",
        "        history.append(f\"<div style='color: green;'>Gemma 3n: {answer_html}</div>\")\n",
        "        history_text = \"<br>\".join(history)\n",
        "\n",
        "        return history_text, answer\n",
        "    except Exception as e:\n",
        "        return history, f\"Error occurred: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "GbbBvHN73O08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    history = gr.State([])\n",
        "    with gr.Column():\n",
        "        gr.Markdown(\"# Welcome to Samir LAHOUAR Portal\")\n",
        "        chat_output = gr.HTML(label=\"Chat History\")\n",
        "        response_output = gr.Textbox(label=\"Response\", placeholder=\"Model response will appear here...\", interactive=False)\n",
        "        url_input = gr.Textbox(lines=1, label=\"Enter File URL (audio/image) or leave it blank\")\n",
        "        question_input = gr.Textbox(lines=2, label=\"Ask Samir's Assistant\")\n",
        "\n",
        "        submit_button = gr.Button(\"Submit\")\n",
        "\n",
        "    # Connect inputs and outputs\n",
        "    submit_button.click(\n",
        "        gemma_chat,\n",
        "        inputs=[history, url_input, question_input],\n",
        "        outputs=[chat_output, response_output]\n",
        "    )"
      ],
      "metadata": {
        "id": "04VNst8x3bcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Launch the Gradio interface\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "yrYx95sS3jNP",
        "outputId": "d28e2dfc-23c6-4dd0-8cfa-ba95d3588042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://4845863e6d8759503c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4845863e6d8759503c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}